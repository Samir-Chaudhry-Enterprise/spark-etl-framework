events {
  process_date = "20200918"
  
  # S3 paths using the specified bucket
  users_input = "s3://samir-apache-spark-demo/input/users"
  events_input = "s3://samir-apache-spark-demo/input/events" 
  train_input = "s3://samir-apache-spark-demo/input/train"
  
  output_dir = "s3://samir-apache-spark-demo/output/features"
  
  db.password = "encrypted_value"
}

application {
  security.decryption.key = "your_decryption_key"
  
  # Scripts location in S3
  scripts_uri = "s3://samir-apache-spark-demo/scripts"
  
  runtime {
    spark {
      app.name = "spark-etl-emr"
      # EMR will set master, don't specify
    }
    
    # S3 Hadoop configuration
    hadoopConfiguration {
      fs.s3a.impl = "org.apache.hadoop.fs.s3a.S3AFileSystem"
      fs.s3a.path.style.access = "true"
      mapreduce.fileoutputcommitter.marksuccessfuljobs = false
    }
    
    filesystem.skip.write.checksum = true
    hiveSupport = false
    validationRun = false
  }
}
