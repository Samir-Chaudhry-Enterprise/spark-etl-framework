# AWS Cloud Configuration for pipeline_fileRead-fileWrite.xml
# This configuration is for running the ETL job on AWS EMR with S3 storage

events {
  process_date = "20200918"

  # S3 paths for input data
  users_input = "s3://samir-apache-spark-demo/v1/input/users"
  events_input = "s3://samir-apache-spark-demo/v1/input/events"
  train_input = "s3://samir-apache-spark-demo/v1/input/train"

  # S3 path for output
  output_dir = "s3://samir-apache-spark-demo/v1/output/features"

  # Passwords should be provided via environment variables or secure configuration
  # db.password = ${?DB_PASSWORD}
  # flight.password = ${?FLIGHT_PASSWORD}
}

kafka {
  bootstrap.servers = "localhost:9092"
  schema.registry.url = "http://localhost:8081"
}

application {
  security.decryption.key = "www.qwshen.com"

  # S3 scripts path
  scripts_uri = "s3://samir-apache-spark-demo/v1/scripts"
  process_date = "2021-02-06"

  runtime {
    spark {
      app.name = "spark-etl-cloud"
      master = "yarn"
    }

    hadoopConfiguration {
      mapreduce.fileoutputcommitter.marksuccessfuljobs = false

      # S3A FileSystem implementation
      fs.s3a.impl = "org.apache.hadoop.fs.s3a.S3AFileSystem"

      # AWS credentials provider - uses environment variables
      fs.s3a.aws.credentials.provider = "com.amazonaws.auth.EnvironmentVariableCredentialsProvider"

      # S3A connection settings
      fs.s3a.connection.maximum = 100

      # S3A fast upload settings for better write performance
      fs.s3a.fast.upload = true
      fs.s3a.fast.upload.buffer = "bytebuffer"

      # S3A multipart upload settings (100MB parts)
      fs.s3a.multipart.size = "104857600"

      # S3A block size (128MB)
      fs.s3a.block.size = "134217728"
    }

    filesystem.skip.write.checksum = true
    hiveSupport = false
    validationRun = false
  }
}

# Staging and metrics paths for cloud (S3)
staging {
  uri = "s3://samir-apache-spark-demo/v1/staging/events"
}

metrics {
  uri = "s3://samir-apache-spark-demo/v1/metrics/events"
}
