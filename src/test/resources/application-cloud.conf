events {
  process_date = "20200918"

  users_input = "s3a://samir-apache-spark-demo/hbase/input/users"
  events_input = "s3a://samir-apache-spark-demo/hbase/input/events"
  train_input = "s3a://samir-apache-spark-demo/hbase/input/train"

  output_dir = "s3a://samir-apache-spark-demo/hbase/output/features"
  delta_output = "s3a://samir-apache-spark-demo/hbase/output/users_delta"

  db.password = ${?DB_PASSWORD}
  flight.password = ${?FLIGHT_PASSWORD}
}

kafka {
  bootstrap.servers = "localhost:9092"
  schema.registry.url = "http://localhost:8081"
}

application {
  security.decryption.key = ${?DECRYPTION_KEY}

  scripts_uri = "."
  process_date = "2021-02-06"

  runtime {
    spark {
      app.name = "spark-etl-cloud"
      master = "yarn"
    }

    hadoopConfiguration {
      mapreduce.fileoutputcommitter.marksuccessfuljobs = false
      fs.s3a.impl = "org.apache.hadoop.fs.s3a.S3AFileSystem"
      fs.s3a.aws.credentials.provider = "com.amazonaws.auth.InstanceProfileCredentialsProvider"
      fs.s3a.endpoint.region = "us-east-2"
      fs.s3a.connection.maximum = 100
      fs.s3a.fast.upload = true
      fs.s3a.fast.upload.buffer = "bytebuffer"
      fs.s3a.multipart.size = "104857600"
      fs.s3a.block.size = "134217728"
    }

    filesystem.skip.write.checksum = true
    hiveSupport = false
    validationRun = false
  }

  staging {
    uri = "s3a://samir-apache-spark-demo/hbase/staging"
  }

  metrics {
    uri = "s3a://samir-apache-spark-demo/hbase/metrics"
  }
}
