# AWS Cloud Configuration for pipeline_fileRead-fileWrite.xml
# This configuration uses S3 paths and S3A optimizations for EMR execution

events {
  process_date = "20200918"

  users_input = "s3://samir-apache-spark-demo/v1/input/users"
  events_input = "s3://samir-apache-spark-demo/v1/input/events"
  train_input = "s3://samir-apache-spark-demo/v1/input/train"

  output_dir = "s3://samir-apache-spark-demo/v1/output"

  # Use environment variables for sensitive values
  db.password = ${?DB_PASSWORD}
  flight.password = ${?FLIGHT_PASSWORD}
}

kafka {
  bootstrap.servers = "localhost:9092"
  schema.registry.url = "http://localhost:8081"
}

application {
  # Use environment variable for decryption key
  security.decryption.key = ${?DECRYPTION_KEY}

  # SQL scripts are distributed via --files, so use current directory
  scripts_uri = "."
  process_date = "2021-02-06"

  runtime {
    spark {
      app.name = "spark-etl-cloud"
      master = "yarn"
    }

    hadoopConfiguration {
      mapreduce.fileoutputcommitter.marksuccessfuljobs = false
      
      # S3A Configuration
      "fs.s3a.impl" = "org.apache.hadoop.fs.s3a.S3AFileSystem"
      "fs.s3a.aws.credentials.provider" = "com.amazonaws.auth.InstanceProfileCredentialsProvider"
      "fs.s3a.endpoint.region" = "us-east-2"
      "fs.s3a.connection.maximum" = 100
      "fs.s3a.fast.upload" = true
      "fs.s3a.fast.upload.buffer" = "bytebuffer"
      "fs.s3a.multipart.size" = "104857600"
      "fs.s3a.block.size" = "134217728"
    }

    filesystem.skip.write.checksum = true
    hiveSupport = false
    validationRun = false
  }
}
