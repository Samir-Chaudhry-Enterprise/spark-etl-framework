events {
  process_date = "20200918"

  users_input = "s3://samir-apache-spark-demo/v1/input/users"
  events_input = "s3://samir-apache-spark-demo/v1/input/events"
  train_input = "s3://samir-apache-spark-demo/v1/input/train"

  output_dir = "s3://samir-apache-spark-demo/v1/output/features"
}

kafka {
  bootstrap.servers = "localhost:9092"
  schema.registry.url = "http://localhost:8081"
}

application {
  # Decryption key for test encrypted values - same as application-test.conf
  security.decryption.key = ${?DECRYPTION_KEY}

  # SQL scripts are distributed via --files, so reference them from current directory
  scripts_uri = "."
  process_date = "2021-02-06"

  runtime {
    spark {
      app.name = "spark-etl-cloud"
      master = "yarn"
    }

    hadoopConfiguration {
      mapreduce.fileoutputcommitter.marksuccessfuljobs = false

      # S3A filesystem configuration
      fs.s3a.impl = "org.apache.hadoop.fs.s3a.S3AFileSystem"
      fs.s3a.aws.credentials.provider = "com.amazonaws.auth.InstanceProfileCredentialsProvider"
      fs.s3a.endpoint.region = "us-east-2"

      # S3A performance optimizations
      fs.s3a.connection.maximum = 100
      fs.s3a.fast.upload = true
      fs.s3a.fast.upload.buffer = "bytebuffer"
      fs.s3a.multipart.size = "104857600"
      fs.s3a.block.size = "134217728"
    }

    filesystem.skip.write.checksum = true
    hiveSupport = false
    validationRun = false
  }
}

# Staging and metrics paths for cloud
staging {
  uri = "s3://samir-apache-spark-demo/v1/staging"
}

metrics {
  uri = "s3://samir-apache-spark-demo/v1/metrics"
}
