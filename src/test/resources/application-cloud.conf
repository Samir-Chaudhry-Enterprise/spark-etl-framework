# Cloud Configuration for AWS S3
# This configuration file is used for running the pipeline in cloud environments

# AWS S3 Data Paths
events {
  process_date = "20200918"

  users_input = "s3://samir-apache-spark-demo/v1/input/users"
  events_input = "s3://samir-apache-spark-demo/v1/input/events"
  train_input = "s3://samir-apache-spark-demo/v1/input/train"

  output_dir = "s3://samir-apache-spark-demo/v1/output/features"
}

application {
  scripts_uri = "s3://samir-apache-spark-demo/v1/scripts"
  process_date = "2021-02-06"

  runtime {
    # Spark configurations for cloud deployment
    spark.app.name = "event-consolidation-cloud"
    
    # Cloud-specific Spark optimizations
    # Dynamic partition overwrite - only overwrites partitions with new data
    spark.sql.sources.partitionOverwriteMode = "dynamic"
    
    # Adaptive Query Execution (Spark 3.x) - optimizes queries at runtime
    spark.sql.adaptive.enabled = "true"
    spark.sql.adaptive.coalescePartitions.enabled = "true"
    spark.sql.adaptive.skewJoin.enabled = "true"
    
    # Serialization optimization
    spark.serializer = "org.apache.spark.serializer.KryoSerializer"
    
    # Speculative execution - helps with stragglers on cloud
    spark.speculation = "true"
    spark.speculation.multiplier = "1.5"
    spark.speculation.quantile = "0.9"
    
    # S3A configurations for AWS access
    # EMR automatically configures S3 access via IAM roles
    hadoopConfiguration {
      # Disable checksum and success markers for cleaner output
      mapreduce.fileoutputcommitter.marksuccessfuljobs = false
    }
    
    filesystem.skip.write.checksum = true
    
    # Validation run setting (set to false for production)
    validationRun = "false"
  }
}

# Staging and metrics paths for cloud
staging {
  uri = "s3://samir-apache-spark-demo/v1/staging"
}

metrics {
  uri = "s3://samir-apache-spark-demo/v1/metrics"
}
