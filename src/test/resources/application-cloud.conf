events {
  process_date = "20200918"

  users_input = "s3a://samir-apache-spark-demo/v1/input/users"
  events_input = "s3a://samir-apache-spark-demo/v1/input/events"
  train_input = "s3a://samir-apache-spark-demo/v1/input/train"

  output_dir = "s3a://samir-apache-spark-demo/v1/output"

  # Encrypted passwords - use environment variables or IAM roles on EMR
  db.password = ${?EVENTS_DB_PASSWORD}
  flight.password = ${?EVENTS_FLIGHT_PASSWORD}
}

kafka {
  bootstrap.servers = "localhost:9092"
  schema.registry.url = "http://localhost:8081"
}

application {
  security.decryption.key = "www.qwshen.com"

  scripts_uri = "s3a://samir-apache-spark-demo/v1/scripts"
  process_date = "2021-02-06"

  runtime {
    spark {
      app.name = "spark-etl-cloud"
      master = "yarn"
    }

    hadoopConfiguration {
      mapreduce.fileoutputcommitter.marksuccessfuljobs = false

      # S3A FileSystem implementation
      fs.s3a.impl = "org.apache.hadoop.fs.s3a.S3AFileSystem"

      # AWS credentials provider - uses environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
      fs.s3a.aws.credentials.provider = "com.amazonaws.auth.EnvironmentVariableCredentialsProvider"

      # S3A connection settings
      fs.s3a.connection.maximum = 100

      # S3A fast upload settings for improved write performance
      fs.s3a.fast.upload = true
      fs.s3a.fast.upload.buffer = "bytebuffer"

      # S3A multipart upload settings
      fs.s3a.multipart.size = "104857600"

      # S3A block size
      fs.s3a.block.size = "134217728"

      # S3A path style access (for compatibility)
      fs.s3a.path.style.access = true

      # S3A endpoint (default AWS S3)
      fs.s3a.endpoint = "s3.amazonaws.com"
    }

    filesystem.skip.write.checksum = true
    hiveSupport = false
    validationRun = false
  }
}

staging {
  uri = "s3a://samir-apache-spark-demo/v1/staging"
}

metrics {
  uri = "s3a://samir-apache-spark-demo/v1/metrics"
}
