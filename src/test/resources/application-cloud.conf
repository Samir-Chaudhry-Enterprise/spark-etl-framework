# Cloud Configuration for AWS S3
# This configuration file is used for running the pipeline in cloud environments

# AWS S3 Data Paths
events {
  process_date = "20200918"

  users_input = "s3://samir-apache-spark-demo/v1/input/users"
  events_input = "s3://samir-apache-spark-demo/v1/input/events"
  train_input = "s3://samir-apache-spark-demo/v1/input/train"

  output_dir = "s3://samir-apache-spark-demo/v1/output/features"
}

application {
  scripts_uri = "s3://samir-apache-spark-demo/v1/scripts"
  process_date = "2021-02-06"

  runtime {
    # Spark configurations for cloud deployment
    spark.app.name = "event-consolidation-cloud"
    
    # Cloud-specific Spark optimizations
    # Dynamic partition overwrite - only overwrites partitions with new data
    spark.sql.sources.partitionOverwriteMode = "dynamic"
    
    # Adaptive Query Execution (Spark 3.x) - optimizes queries at runtime
    spark.sql.adaptive.enabled = "true"
    spark.sql.adaptive.coalescePartitions.enabled = "true"
    spark.sql.adaptive.skewJoin.enabled = "true"
    
    # Serialization optimization
    spark.serializer = "org.apache.spark.serializer.KryoSerializer"
    
    # Speculative execution - helps with stragglers on cloud
    spark.speculation = "true"
    spark.speculation.multiplier = "1.5"
    spark.speculation.quantile = "0.9"
    
    # S3A configurations for AWS access
    # Note: Credentials should be provided via environment variables or IAM roles
    # AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables
    # or use IAM roles when running on EMR/EKS
    hadoopConfiguration {
      # S3A filesystem configuration
      fs.s3a.impl = "org.apache.hadoop.fs.s3a.S3AFileSystem"
      fs.s3a.aws.credentials.provider = "com.amazonaws.auth.EnvironmentVariableCredentialsProvider"
      
      # Performance optimizations for S3
      fs.s3a.connection.maximum = 100
      fs.s3a.fast.upload = true
      fs.s3a.fast.upload.buffer = "bytebuffer"
      fs.s3a.multipart.size = "104857600"
      fs.s3a.block.size = "134217728"
      
      # S3A Committer for better write performance and consistency
      fs.s3a.committer.name = "magic"
      fs.s3a.committer.magic.enabled = "true"
      mapreduce.outputcommitter.factory.scheme.s3a = "org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory"
      
      # Disable checksum for better performance
      mapreduce.fileoutputcommitter.marksuccessfuljobs = false
    }
    
    filesystem.skip.write.checksum = true
    
    # Validation run setting (set to false for production)
    validationRun = "false"
  }
}

# Staging and metrics paths for cloud
staging {
  uri = "s3://samir-apache-spark-demo/v1/staging"
}

metrics {
  uri = "s3://samir-apache-spark-demo/v1/metrics"
}
