events {
  process_date = "20200918"

  users_input = "s3a://samir-apache-spark-demo/hbase/v1/input/users"
  delta_output = "s3a://samir-apache-spark-demo/hbase/v1/output/users_delta"
  staging_uri = "s3a://samir-apache-spark-demo/hbase/v1/staging"

  db.password = ${?DB_PASSWORD}
  flight.password = ${?FLIGHT_PASSWORD}
}

application {
  security.decryption.key = ${?DECRYPTION_KEY}

  scripts_uri = "."
  process_date = "2021-02-06"

  runtime {
    spark {
      app.name = "spark-etl-cloud"
      master = "yarn"
    }

    hadoopConfiguration {
      mapreduce.fileoutputcommitter.marksuccessfuljobs = false
      fs.s3a.impl = "org.apache.hadoop.fs.s3a.S3AFileSystem"
      fs.s3a.aws.credentials.provider = "com.amazonaws.auth.InstanceProfileCredentialsProvider"
      fs.s3a.endpoint.region = "us-east-2"
      fs.s3a.connection.maximum = 100
      fs.s3a.fast.upload = true
      fs.s3a.fast.upload.buffer = "bytebuffer"
      fs.s3a.multipart.size = "104857600"
      fs.s3a.block.size = "134217728"
    }

    filesystem.skip.write.checksum = true
    hiveSupport = false
    validationRun = false
  }
}
