# On-Prem Configuration for pipeline_fileRead-fileWrite.xml
# This configuration is for running the ETL job locally with local filesystem paths

events {
  process_date = "20200918"

  # Local filesystem paths for input data
  users_input = "./data/users"
  events_input = "./data/events"
  train_input = "./data/train"

  # Local filesystem path for output
  output_dir = "./data/features"

  # Passwords should be provided via environment variables or secure configuration
  # db.password = ${?DB_PASSWORD}
  # flight.password = ${?FLIGHT_PASSWORD}
}

kafka {
  bootstrap.servers = "localhost:9092"
  schema.registry.url = "http://localhost:8081"
}

application {
  security.decryption.key = "www.qwshen.com"

  # Local scripts path
  scripts_uri = "./scripts"
  process_date = "2021-02-06"

  runtime {
    spark {
      app.name = "spark-etl-onprem"
      master = "local[*]"
    }

    hadoopConfiguration {
      mapreduce.fileoutputcommitter.marksuccessfuljobs = false
    }

    filesystem.skip.write.checksum = true
    hiveSupport = false
    validationRun = false
  }
}

# Staging and metrics paths for on-prem
staging {
  uri = "/tmp/staging/events"
}

metrics {
  uri = "/tmp/metrics/events"
}
